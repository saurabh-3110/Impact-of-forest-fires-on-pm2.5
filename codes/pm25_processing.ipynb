{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###########################################################################################################################\n",
    "# Import file from the following link: https://zenodo.org/communities/ecnu_lghap/records?q=&l=list&p=1&s=10&sort=newest \n",
    "###########################################################################################################################\n",
    "\n",
    "# === Step 1: Load and coarsen dataset ===\n",
    "file_path = '/Users/saurabh/Desktop/foresaerosol_new/LGHAP.Global_PM25.D001.A20190201.nc'\n",
    "ds = xr.open_dataset(file_path)\n",
    "ds_india = ds.sel(lat=slice(8, 38), lon=slice(68, 98))\n",
    "coarse_ds = ds_india.coarsen(lat=10, lon=10, boundary='trim').mean()\n",
    "pm25 = coarse_ds['PM25'].transpose('lat', 'lon')\n",
    "df_pm25 = pm25.to_dataframe().reset_index()\n",
    "\n",
    "\n",
    "df_pm25['lat'] = df_pm25['lat'].astype(float).round(2)\n",
    "df_pm25['lon'] = df_pm25['lon'].astype(float).round(2)\n",
    "\n",
    "# === Step 2: Filter grid points within shapefile boundary ===\n",
    "# Load shapefile\n",
    "gdf_boundary = gpd.read_file(\"/Users/saurabh/Desktop/foresaerosol_new/shapefiles/center_shp/center.shp\")\n",
    "\n",
    "# Ensure CRS is EPSG:4326\n",
    "if gdf_boundary.crs is None:\n",
    "    gdf_boundary.set_crs(\"EPSG:4326\", inplace=True)\n",
    "else:\n",
    "    gdf_boundary = gdf_boundary.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Load grid points and convert to GeoDataFrame\n",
    "df_grids = pd.read_csv(\"/Users/saurabh/Desktop/foresaerosol_new/center_study_area/forest_fire_processing/grids_10km_india_box.csv\")\n",
    "geometry = [Point(xy) for xy in zip(df_grids['lon'], df_grids['lat'])]\n",
    "gdf_points = gpd.GeoDataFrame(df_grids, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Spatial join: keep only points within the shapefile boundary\n",
    "gdf_inside = gpd.sjoin(gdf_points, gdf_boundary, predicate='within', how='inner')\n",
    "study_grids = gdf_inside.drop(columns=['geometry', 'index_right'])\n",
    "\n",
    "study_grids['lat'] = study_grids['lat'].astype(float).round(2)\n",
    "study_grids['lon'] = study_grids['lon'].astype(float).round(2)\n",
    "\n",
    "# === Step 3: Merge PM2.5 with filtered grids ===\n",
    "merged_df = pd.merge(df_pm25, study_grids, on=['lat', 'lon'])\n",
    "\n",
    "# Keep only needed columns\n",
    "filtered_df = merged_df[['lat', 'lon', 'PM25']]\n",
    "\n",
    "# Save\n",
    "filtered_df.to_csv('filtered_pm25_data.csv', index=False)\n",
    "\n",
    "# === Step 4: Visualization ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    filtered_df['lon'], filtered_df['lat'],\n",
    "    c=filtered_df['PM25'], cmap='inferno', s=30, edgecolor='k'\n",
    ")\n",
    "plt.colorbar(scatter, label='PM2.5 Concentration (µg/m³)')\n",
    "plt.title('PM2.5 Concentration Over Study Area')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "###########################################################################################################################\n",
    "# Import files from the following link: https://zenodo.org/communities/ecnu_lghap/records?q=&l=list&p=1&s=10&sort=newest \n",
    "# Save the file of the months required in the input_root directory.\n",
    "###########################################################################################################################\n",
    "\n",
    "# Paths\n",
    "input_root = 'raw_files'\n",
    "output_root = 'monthly_output'\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# Load shapefile once. ############ Change the path to the shapefile as per requirement ##########\n",
    "shapefile_path = \"center_shp/center.shp\"\n",
    "gdf_boundary = gpd.read_file(shapefile_path)\n",
    "if gdf_boundary.crs is None:\n",
    "    gdf_boundary.set_crs(\"EPSG:4326\", inplace=True)\n",
    "else:\n",
    "    gdf_boundary = gdf_boundary.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Load grid points once\n",
    "grids_path = \"/Users/saurabh/Desktop/projects/foresaerosol_new/center_study_area/forest_fire_processing/grids_10km_india_box.csv\"\n",
    "df_grids = pd.read_csv(grids_path)\n",
    "geometry = [Point(xy) for xy in zip(df_grids['lon'], df_grids['lat'])]\n",
    "gdf_points = gpd.GeoDataFrame(df_grids, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Filter grid points within shapefile boundary once\n",
    "gdf_inside = gpd.sjoin(gdf_points, gdf_boundary, predicate='within', how='inner')\n",
    "study_grids = gdf_inside.drop(columns=['geometry', 'index_right'])\n",
    "study_grids['lat'] = study_grids['lat'].astype(float).round(2)\n",
    "study_grids['lon'] = study_grids['lon'].astype(float).round(2)\n",
    "\n",
    "# Define India bounding box\n",
    "lat_slice = slice(8, 38)\n",
    "lon_slice = slice(68, 98)\n",
    "\n",
    "# Process all monthly folders\n",
    "for monthly_folder in tqdm(sorted(os.listdir(input_root))):\n",
    "    monthly_path = os.path.join(input_root, monthly_folder)\n",
    "    if not os.path.isdir(monthly_path):\n",
    "        continue\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for nc_file in sorted(os.listdir(monthly_path)):\n",
    "        if not nc_file.endswith('.nc'):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(monthly_path, nc_file)\n",
    "\n",
    "        # Extract date from filename\n",
    "        match = re.search(r'A(\\d{8})', nc_file)\n",
    "        if match:\n",
    "            date_str = match.group(1)\n",
    "            date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load and coarsen dataset\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            ds_india = ds.sel(lat=lat_slice, lon=lon_slice)\n",
    "            coarse_ds = ds_india.coarsen(lat=10, lon=10, boundary='trim').mean()\n",
    "            pm25 = coarse_ds['PM25'].transpose('lat', 'lon')\n",
    "            df_pm25 = pm25.to_dataframe().reset_index()\n",
    "            df_pm25['lat'] = df_pm25['lat'].astype(float).round(2)\n",
    "            df_pm25['lon'] = df_pm25['lon'].astype(float).round(2)\n",
    "\n",
    "            # Step 2: Merge PM2.5 with filtered grids\n",
    "            merged_df = pd.merge(df_pm25, study_grids, on=['lat', 'lon'])\n",
    "\n",
    "            # Add date column\n",
    "            merged_df.insert(0, 'date', date_obj)\n",
    "\n",
    "            # Keep only needed columns\n",
    "            filtered_df = merged_df[['date', 'lat', 'lon', 'PM25']]\n",
    "            all_dfs.append(filtered_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        monthly_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        out_filename = f\"{monthly_folder}.csv\"\n",
    "        out_path = os.path.join(output_root, out_filename)\n",
    "        monthly_df.to_csv(out_path, index=False)\n",
    "        print(f\"Saved {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "output_root = 'monthly_output'\n",
    "\n",
    "combined_output_path = 'combined_pm25_data.csv'\n",
    "\n",
    "# List all CSV files in the output directory\n",
    "csv_files = sorted([f for f in os.listdir(output_root) if f.endswith('.csv')])\n",
    "\n",
    "# Read and concatenate all monthly files\n",
    "all_monthly_dfs = []\n",
    "\n",
    "for file in tqdm(csv_files):\n",
    "    file_path = os.path.join(output_root, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_monthly_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Combine into one DataFrame\n",
    "if all_monthly_dfs:\n",
    "    combined_df = pd.concat(all_monthly_dfs, ignore_index=True)\n",
    "    combined_df.to_csv(combined_output_path, index=False)\n",
    "    print(f\"Combined file saved at: {combined_output_path}\")\n",
    "else:\n",
    "    print(\"No data found to combine.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
